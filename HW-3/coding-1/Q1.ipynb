{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (a) load data\n",
    "def load_data(filename):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for line in open(\"sentiment labelled sentences/\" + filename):\n",
    "        sentences.append(line.strip('\\n').split('\\t')[0])\n",
    "        labels.append(line.strip('\\n').split('\\t')[1])\n",
    "#     print(\"dataset: \"+str(filename))\n",
    "#     print(\"total sentences: \" + str(len(labels)))\n",
    "#     print(\"positive sentences: \" + str(labels.count('1')))\n",
    "#     print(\"negative sentences: \" + str(labels.count('0')))\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (b) preprocessing\n",
    "def preprocessing(sentences, lowercase, strip_punctuation, lemmatization, remove_stop_words):\n",
    "    trainset = sentences\n",
    "    \n",
    "    if lowercase:\n",
    "        # lowercase all of the words\n",
    "        #print(\"preprocessing - lowercase\")\n",
    "        trainset_0 = []\n",
    "        for sentence in trainset:\n",
    "            trainset_0.append(sentence.lower())\n",
    "        trainset = trainset_0\n",
    "        #print(trainset[0])\n",
    "    \n",
    "    if strip_punctuation:\n",
    "        # strip punctuation\n",
    "        #print(\"preprocessing - strip punctuation\")\n",
    "        trainset_0 = []\n",
    "        for sentence in trainset:\n",
    "            trainset_0.append(sentence.translate(str.maketrans('', '', string.punctuation)))\n",
    "        trainset = trainset_0\n",
    "        #print(trainset[0])\n",
    "    \n",
    "    if lemmatization:\n",
    "        # lemmatization of all the words\n",
    "        #print(\"preprocessing - lemmatization\")\n",
    "        ps = PorterStemmer()\n",
    "        trainset_0 = []\n",
    "        for sentence in trainset:\n",
    "            words = sentence.split(' ')\n",
    "            sentence_out = \"\"\n",
    "            for word in words:\n",
    "                sentence_out = sentence_out + ps.stem(word) + ' '\n",
    "            trainset_0.append(sentence_out)\n",
    "        trainset = trainset_0\n",
    "        #print(trainset[0])\n",
    "    \n",
    "    if remove_stop_words:\n",
    "        # remove stop words\n",
    "        #print(\"preprocessing - remove stop words\")\n",
    "        trainset_0 = []\n",
    "        #stop_words = set(stopwords.words('english'))\n",
    "        stop_words = set(['the','and','or','a','an','for','that','is','are','were','was'])\n",
    "        for sentence in trainset:\n",
    "            word_tokens = word_tokenize(sentence) \n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            trainset_0.append(\" \".join(filtered_sentence))\n",
    "            #print(\" \".join(filtered_sentence))\n",
    "        trainset = trainset_0\n",
    "        #print(trainset[0])\n",
    "    \n",
    "    # split sentences\n",
    "    for i in range(len(trainset)):\n",
    "        trainset[i] = trainset[i].split(' ')\n",
    "    #print(trainset[0])\n",
    "    \n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (c) split training and testing set\n",
    "def split_data(sentences, labels):\n",
    "    one_indexs = [i for i,x in enumerate(labels) if x == '1']\n",
    "    zero_indexs = [i for i,x in enumerate(labels) if x == '0']\n",
    "    trainset_1 = [sentence for i,sentence in enumerate(sentences) if i in one_indexs[:400]]\n",
    "    trainset_0 = [sentence for i,sentence in enumerate(sentences) if i in zero_indexs[:400]]\n",
    "    testset_1 = [sentence for i,sentence in enumerate(sentences) if i in one_indexs[400:]]\n",
    "    testset_0 = [sentence for i,sentence in enumerate(sentences) if i in zero_indexs[400:]]\n",
    "    trainset = trainset_1 + trainset_0\n",
    "    testset = testset_1 + testset_0\n",
    "    trainlabels = np.concatenate((np.ones((400,1)),np.zeros((400,1))), axis = 0)\n",
    "    testlabels = np.concatenate((np.ones((100,1)),np.zeros((100,1))), axis = 0)\n",
    "    return trainset, testset, trainlabels, testlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trainset_testset():\n",
    "    trainset = []\n",
    "    testset = []\n",
    "    trainlabels = []\n",
    "    testlabels = []\n",
    "    for filename in [\"amazon_cells_labelled.txt\", \"imdb_labelled.txt\", \"yelp_labelled.txt\"]:\n",
    "        sentences, labels = load_data(filename)\n",
    "        sentences = preprocessing(sentences,1,1,1,1)\n",
    "        trainset0, testset0, trainlabels0, testlabels0 = split_data(sentences, labels)\n",
    "        trainset.extend(trainset0)\n",
    "        testset.extend(testset0)\n",
    "        trainlabels.extend(trainlabels0)\n",
    "        testlabels.extend(testlabels0)\n",
    "    return trainset, testset, trainlabels, testlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (d) Bag of words model - extract features\n",
    "def bag_of_words():\n",
    "    trainset, testset, trainlabels, testlabels = generate_trainset_testset()\n",
    "\n",
    "    # build dictionary of unique words\n",
    "    dict_key = []\n",
    "    for sentence in trainset:\n",
    "        dict_key.extend(sentence)\n",
    "    dict_key = list(set(dict_key))\n",
    "    # my_dict = my_dict.fromkeys(dict_key, 0)\n",
    "\n",
    "    # extract features\n",
    "    train_features = np.zeros((len(trainset), len(dict_key)))\n",
    "    for i in range(len(trainset)):\n",
    "        for word in trainset[i]:\n",
    "            train_features[i, dict_key.index(word)] = train_features[i, dict_key.index(word)] + 1\n",
    "\n",
    "    test_features = np.zeros((len(testset), len(dict_key)))\n",
    "    for i in range(len(testset)):\n",
    "        for word in testset[i]:\n",
    "            if word in dict_key:\n",
    "                test_features[i, dict_key.index(word)] = test_features[i, dict_key.index(word)] + 1\n",
    "\n",
    "    return dict_key, train_features, test_features, trainlabels, testlabels\n",
    "    \n",
    "#     print(\"shape of train_features: \" + str(np.shape(train_features)))\n",
    "#     print(\"shape of test_features: \"+ str(np.shape(test_features)))\n",
    "\n",
    "#     print(\"sample trainset review 0:\")\n",
    "#     print(\"sum of feature vector: \" + str(sum(train_features[0,:])))\n",
    "#     print(\"indexs of non zero element in feature vector: \" + str([i for i,x in enumerate(train_features[0,:]) if x != 0]))\n",
    "#     print(\"non zero words: \" + str([x for i,x in enumerate(dict_key) if train_features[0,i] != 0]))\n",
    "\n",
    "#     print(\"sample trainset review 3:\")\n",
    "#     print(\"sum of feature vector: \" + str(sum(train_features[3,:])))\n",
    "#     print(\"indexs of non zero element in feature vector: \" + str([i for i,x in enumerate(train_features[3,:]) if x != 0]))\n",
    "#     print(\"non zero words: \" + str([x for i,x in enumerate(dict_key) if train_features[3,i] != 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (e) Post-processing\n",
    "def L2_norm(features):\n",
    "    post_features = features\n",
    "    for i in range(np.shape(post_features)[0]):\n",
    "        #print(np.sum(post_features[i,:]))\n",
    "        #print(linalg.norm(post_features[i,:]))\n",
    "        post_features[i,:] = post_features[i,:] / linalg.norm(post_features[i,:])\n",
    "        #print(features[i,:])\n",
    "    return post_features\n",
    "\n",
    "def standardize(features):\n",
    "    post_features = features\n",
    "    post_features = post_features - np.mean(post_features,axis=0)\n",
    "    feat_var = np.var(post_features, axis=0)\n",
    "    for i in range(np.shape(post_features)[1]):\n",
    "        post_features[:,i] = post_features[:,i] / feat_var[i]\n",
    "        # print(feat_var[i])\n",
    "    return post_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (f) sentiment prediction\n",
    "def sentiment_prediction(dict_key, md, train_features,trainlabels, test_features, testlabels):\n",
    "    if md == \"NB\":\n",
    "        model = GaussianNB()\n",
    "    elif md == \"LR\":\n",
    "        model = LR()\n",
    "    clf = model.fit(train_features, np.array(trainlabels))\n",
    "    pred_labels = clf.predict(test_features).reshape(600,1)\n",
    "    # print(sum(abs(np.array(clf.predict(test_features)).reshape(600,1) - np.array(testlabels))))\n",
    "    print(\"score: \" + str(clf.score(test_features, np.array(testlabels))))\n",
    "\n",
    "    cm = confusion_matrix(testlabels, pred_labels, labels=[1,0])\n",
    "    print(\"confusion matrix:\")\n",
    "    print(\"      pred_1,pred_0\")\n",
    "    print(\"true_1  \" + str(cm[0,:]))\n",
    "    print(\"true_0  \" + str(cm[1,:]))\n",
    "    print(\"TPR=\" + str(cm[0,0]/300))\n",
    "    print(\"FPR=\" + str(cm[1,0]/300))\n",
    "\n",
    "    if md == \"LR\":\n",
    "        sorted_w_ind = np.argsort(-abs(clf.coef_[0,:]))\n",
    "        print()\n",
    "        for i in sorted_w_ind[:10]:\n",
    "            print(dict_key[i], clf.coef_[0,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (g) N-gram model\n",
    "def n_gram(n):\n",
    "    trainset, testset, trainlabels, testlabels = generate_trainset_testset()\n",
    "    \n",
    "    # build dictionary of n-grams\n",
    "    dict_key = []\n",
    "    for sentence in trainset:\n",
    "        if len(sentence) >= n:\n",
    "            for i in range(len(sentence) - n + 1):\n",
    "                dict_key.append(\" \".join(sentence[i:i+n]))\n",
    "    dict_key = list(set(dict_key))\n",
    "\n",
    "    # extract features\n",
    "    train_features = np.zeros((len(trainset), len(dict_key)))\n",
    "    for i in range(len(trainset)):\n",
    "        sentence = trainset[i]\n",
    "        if len(sentence) >= n:\n",
    "            for j in range(len(sentence) - n + 1):\n",
    "                feat = \" \".join(sentence[j:j+n])\n",
    "                train_features[i, dict_key.index(feat)] = train_features[i, dict_key.index(feat)] + 1\n",
    "    \n",
    "    test_features = np.zeros((len(testset), len(dict_key)))\n",
    "    for i in range(len(testset)):\n",
    "        sentence = testset[i]\n",
    "        if len(sentence) >= n:\n",
    "            for j in range(len(sentence) - n + 1):\n",
    "                feat = \" \".join(sentence[j:j+n])\n",
    "                if feat in dict_key:\n",
    "                    test_features[i, dict_key.index(feat)] = test_features[i, dict_key.index(feat)] + 1\n",
    "    \n",
    "    return dict_key, train_features, test_features, trainlabels, testlabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (h) PCA for bag of words model\n",
    "def PCA_bow(features, n):\n",
    "    C = np.dot(features.T, features) / (np.shape(features)[0] - 1)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(C)\n",
    "    print(np.shape(eig_vecs[:,:n]))\n",
    "    pca_features = np.dot(features, eig_vecs[:,:n])   \n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3768, 10)\n",
      "(3768, 10)\n",
      "score: 0.53\n",
      "confusion matrix:\n",
      "      pred_1,pred_0\n",
      "true_1  [212  88]\n",
      "true_0  [194 106]\n",
      "TPR=0.7066666666666667\n",
      "FPR=0.6466666666666666\n",
      "\n",
      "definitli -1.3154851949396043\n",
      "craft 1.2532819341272579\n",
      "understat 0.21787418754472146\n",
      "koteasjack -0.21154135372011537\n",
      "correctli -0.16708561109295955\n",
      "belmondo -0.14152200704163495\n",
      "price -0.11000040851583964\n",
      "soundtrack -0.0613655886499206\n",
      "through 0.039622060793330846\n",
      "muffl 0.03655618193186981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:433: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "dict_key, train_features, test_features, trainlabels, testlabels = bag_of_words()\n",
    "pca_train_features = PCA_bow(train_features, 10)\n",
    "pca_test_features = PCA_bow(test_features, 10)\n",
    "sentiment_prediction(dict_key, \"LR\", pca_train_features,trainlabels, pca_test_features, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:433: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.4683333333333333\n",
      "confusion matrix:\n",
      "      pred_1,pred_0\n",
      "true_1  [123 177]\n",
      "true_0  [142 158]\n",
      "TPR=0.41\n",
      "FPR=0.47333333333333333\n"
     ]
    }
   ],
   "source": [
    "pca_train_features = PCA_bow(train_features, 50)\n",
    "pca_test_features = PCA_bow(test_features, 50)\n",
    "sentiment_prediction(dict_key, \"LR\", pca_train_features,trainlabels, pca_test_features, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.4666666666666667\n",
      "confusion matrix:\n",
      "      pred_1,pred_0\n",
      "true_1  [119 181]\n",
      "true_0  [139 161]\n",
      "TPR=0.39666666666666667\n",
      "FPR=0.4633333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:433: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n",
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "pca_train_features = PCA_bow(train_features, 100)\n",
    "pca_test_features = PCA_bow(test_features, 100)\n",
    "sentiment_prediction(dict_key, \"LR\", pca_train_features,trainlabels, pca_test_features, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.835\n",
      "confusion matrix:\n",
      "      pred_1,pred_0\n",
      "true_1  [237  63]\n",
      "true_0  [ 36 264]\n",
      "TPR=0.79\n",
      "FPR=0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# dict_key, train_features, test_features, trainlabels, testlabels = n_gram(2)\n",
    "# dict_key, trainset, testset, trainlabels, testlabels = bag_of_words()\n",
    "sentiment_prediction(dict_key, \"LR\", train_features,trainlabels, test_features, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 100)\n",
      "score: 0.4866666666666667\n",
      "confusion matrix:\n",
      "      pred_1,pred_0\n",
      "true_1  [148 152]\n",
      "true_0  [156 144]\n",
      "TPR=0.49333333333333335\n",
      "FPR=0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_train_features = PCA(n_components=100).fit_transform(train_features)\n",
    "pca_test_features = PCA(n_components=100).fit_transform(test_features)\n",
    "sentiment_prediction(dict_key, \"LR\", pca_train_features,trainlabels, pca_test_features, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.6783333333333333\n",
      "confusion matrix:\n",
      "      pred_1,pred_0\n",
      "true_1  [236  64]\n",
      "true_0  [129 171]\n",
      "TPR=0.7866666666666666\n",
      "FPR=0.43\n",
      "\n",
      "work great 1.7644207945329984\n",
      "veri disappoint -1.637707140844255\n",
      "i love 1.5377370549922356\n",
      "not good -1.4864384148992145\n",
      "veri good 1.4742494746212105\n",
      "would not -1.3969040749585848\n",
      "do not -1.3517816999574568\n",
      "highli recommend 1.3436585474566296\n",
      "i like 1.2566762569451877\n",
      "piec of -1.2183069808315363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "dict_key, train_features, test_features, trainlabels, testlabels = n_gram(2)\n",
    "# dict_key, trainset, testset, trainlabels, testlabels = bag_of_words()\n",
    "sentiment_prediction(dict_key, \"LR\", train_features,trainlabels, test_features, testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
